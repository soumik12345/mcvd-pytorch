{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b8ed9c-ba27-440c-9549-ded67739bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "\n",
    "import glob\n",
    "import yaml\n",
    "import wandb\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from imageio import mimwrite\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import make_grid, save_image\n",
    "try:\n",
    "    from torchvision.transforms.functional import resize, InterpolationMode\n",
    "    interp = InterpolationMode.NEAREST\n",
    "except:\n",
    "    from torchvision.transforms.functional import resize\n",
    "    interp = 0\n",
    "\n",
    "from main import dict2namespace\n",
    "from models.ema import EMAHelper\n",
    "import models.eval_models as eval_models\n",
    "from models import (\n",
    "    get_sigmas,\n",
    "    anneal_Langevin_dynamics,\n",
    "    anneal_Langevin_dynamics_consistent,\n",
    "    ddpm_sampler,\n",
    "    ddim_sampler,\n",
    "    FPNDM_sampler\n",
    ")\n",
    "from runners.ncsn_runner import get_model, conditioning_fn\n",
    "from load_model_from_ckpt import load_model, get_sampler, init_samples\n",
    "from datasets import get_dataset, data_transform, inverse_data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d13f9c-1e56-493e-9087-2e4d99eb3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.channels = 1\n",
    "    config.dataset = 'StochasticMovingMNIST'\n",
    "    config.gaussian_dequantization = False\n",
    "    config.image_size = 64\n",
    "    config.logit_transform = False\n",
    "    config.num_digits = 2\n",
    "    config.num_frames = 5\n",
    "    config.num_frames_cond = 5\n",
    "    config.num_frames_future = 0\n",
    "    config.num_workers = 0\n",
    "    config.prob_mask_cond = 0.0\n",
    "    config.prob_mask_future = 0.0\n",
    "    config.prob_mask_sync = False\n",
    "    config.random_flip = True\n",
    "    config.rescaled = True\n",
    "    config.step_length = 0.1\n",
    "    config.uniform_dequantization = False\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_fast_fid_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.batch_size = 1000\n",
    "    config.begin_ckpt = 5000\n",
    "    config.end_ckpt = 300000\n",
    "    config.ensemble = False\n",
    "    config.freq = 5000\n",
    "    config.n_steps_each = 0\n",
    "    config.num_samples = 1000\n",
    "    config.pr_nn_k = 3\n",
    "    config.step_lr = 0.0\n",
    "    config.verbose = False\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_model_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.arch = 'unetmore'\n",
    "    config.attn_resolutions = [8, 16, 32]\n",
    "    config.ch_mult = [1, 2, 3, 4]\n",
    "    config.cond_emb = False\n",
    "    config.conditional = True\n",
    "    config.depth = 'deep'\n",
    "    config.dropout = 0.1\n",
    "    config.ema = True\n",
    "    config.ema_rate = 0.999\n",
    "    config.gamma = False\n",
    "    config.n_head_channels = 64\n",
    "    config.ngf = 64\n",
    "    config.noise_in_cond = False\n",
    "    config.nonlinearity = 'swish'\n",
    "    config.normalization = 'InstanceNorm++'\n",
    "    config.num_classes = 1000\n",
    "    config.num_res_blocks = 2\n",
    "    config.output_all_frames = False\n",
    "    config.sigma_begin = 0.02\n",
    "    config.sigma_dist = 'linear'\n",
    "    config.sigma_end = 0.0001\n",
    "    config.spade = False\n",
    "    config.spade_dim = 128\n",
    "    config.spec_norm = False\n",
    "    config.time_conditional = True\n",
    "    config.type = 'v1'\n",
    "    config.scheduler = 'DDPM'\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_optim_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.amsgrad = False\n",
    "    config.beta1 = 0.9\n",
    "    config.eps = 1e-08\n",
    "    config.grad_clip = 1.0\n",
    "    config.lr = 0.0002\n",
    "    config.optimizer = 'Adam'\n",
    "    config.warmup = 1000\n",
    "    config.weight_decay = 0.0\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_sampling_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.batch_size = 100\n",
    "    config.ckpt_id = 0\n",
    "    config.clip_before = True\n",
    "    config.consistent = True\n",
    "    config.data_init = False\n",
    "    config.denoise = True\n",
    "    config.fid = False\n",
    "    config.final_only = True\n",
    "    config.fvd = True\n",
    "    config.init_prev_t = -1.0\n",
    "    config.inpainting = False\n",
    "    config.interpolation = False\n",
    "    config.max_data_iter = 100000\n",
    "    config.n_interpolations = 15\n",
    "    config.n_steps_each = 0\n",
    "    config.num_frames_pred = 20\n",
    "    config.num_samples4fid = 10000\n",
    "    config.num_samples4fvd = 10000\n",
    "    config.one_frame_at_a_time = False\n",
    "    config.preds_per_test = 1\n",
    "    config.ssim = True\n",
    "    config.step_lr = 0.0\n",
    "    config.subsample = 1000\n",
    "    config.train = False\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_test_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.batch_size = 100\n",
    "    config.begin_ckpt = 5000\n",
    "    config.end_ckpt = 300000\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_training_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.L1 = False\n",
    "    config.batch_size = 64\n",
    "    config.checkpoint_freq = 100\n",
    "    config.log_all_sigmas = False\n",
    "    config.log_freq = 50\n",
    "    config.n_epochs = 500\n",
    "    config.n_iters = 3000001\n",
    "    config.sample_freq = 50000\n",
    "    config.snapshot_freq = 1000\n",
    "    config.snapshot_sampling = True\n",
    "    config.val_freq = 100\n",
    "    config.checkpoint_dir = \"smmnist_cat\"\n",
    "    config.checkpoint_freq = 50\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_config() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "    \n",
    "    config.data = get_data_configs()\n",
    "    config.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    config.fast_fid = get_fast_fid_configs()\n",
    "    config.model = get_model_configs()\n",
    "    config.optim = get_optim_configs()\n",
    "    config.sampling = get_sampling_configs()\n",
    "    config.test = get_test_configs()\n",
    "    config.training = get_training_configs()\n",
    "    config.start_at = 0\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ede94e-f867-4e3e-95a6-4df7178689ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(arr):\n",
    "    m, M = arr.min(), arr.max()\n",
    "    return (arr - m) / (M - m)\n",
    "\n",
    "\n",
    "def ls(path): \n",
    "    return sorted(list(path.iterdir()))\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b067f0a-baab-4e03-bb07-9635f4a02619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgeekyrakshit\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/mcvd-pytorch/wandb/run-20230130_125830-bhtww66v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/bhtww66v\" target=\"_blank\">virtuous-fuse-93</a></strong> to <a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion\" target=\"_blank\">https://wandb.ai/wandb/masked-conditional-video-diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/bhtww66v\" target=\"_blank\">https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/bhtww66v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact checkpoint-glistening-snake-87-yfq7vyx1:v9, 426.95MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.1\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "config_dict = config.to_dict()\n",
    "config_dict.pop(\"device\", None)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"masked-conditional-video-diffusion\",\n",
    "    entity=\"wandb\", job_type=\"inference\", config=config_dict\n",
    ")\n",
    "\n",
    "artifact = wandb.use_artifact(\n",
    "    'wandb/masked-conditional-video-diffusion/checkpoint-glistening-snake-87-yfq7vyx1:v9', type='model'\n",
    ")\n",
    "model_artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa48f43-6dc1-451d-8cf8-ad5eec566273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path):\n",
    "    scorenet = get_model(config)\n",
    "    if config.device != torch.device('cpu'):\n",
    "        scorenet = torch.nn.DataParallel(scorenet)\n",
    "        states = torch.load(ckpt_path, map_location=config.device)\n",
    "    else:\n",
    "        states = torch.load(ckpt_path, map_location='cpu')\n",
    "        states[0] = OrderedDict([(k.replace('module.', ''), v) for k, v in states[0].items()])\n",
    "    scorenet.load_state_dict(states[0], strict=False)\n",
    "    if config.model.ema:\n",
    "        ema_helper = EMAHelper(mu=config.model.ema_rate)\n",
    "        ema_helper.register(scorenet)\n",
    "        ema_helper.load_state_dict(states[-1])\n",
    "        ema_helper.ema(scorenet)\n",
    "    scorenet.eval()\n",
    "    return scorenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7a1303-7251-4cc9-96de-ff5bb3b03ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(model_artifact_dir, \"checkpoint.pt\")\n",
    "scorenet = load_model(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aec96e99-d602-4e68-9b59-3551bfd517d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudDataset:\n",
    "    \n",
    "    def __init__(self, files, num_frames=4, scale=True, size=64):\n",
    "        self.num_frames = num_frames\n",
    "        self.size = size\n",
    "        self.tfms = T.Compose([\n",
    "            T.Resize((size, int(size * 1.7))),\n",
    "            T.CenterCrop(size)\n",
    "        ])\n",
    "        data = []\n",
    "        for file in tqdm(files):\n",
    "            one_day = np.load(file)\n",
    "            one_day = 0.5 - self._scale(one_day) if scale else one_day\n",
    "            wds = np.lib.stride_tricks.sliding_window_view(\n",
    "                one_day.squeeze(), \n",
    "                num_frames, \n",
    "                axis=0\n",
    "            ).transpose((0, 3, 1, 2))\n",
    "            data.append(wds)\n",
    "        self.data = np.concatenate(data, axis=0)\n",
    "            \n",
    "    @staticmethod\n",
    "    def _scale(arr):\n",
    "        \"Scales values of array in [0,1]\"\n",
    "        m, M = arr.min(), arr.max()\n",
    "        return (arr - m) / (M - m)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.tfms(torch.from_numpy(self.data[idx]))\n",
    "        data = torch.unsqueeze(data, dim=-3)\n",
    "        return data, data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def save(self, fname=\"cloud_frames.npy\"):\n",
    "        np.save(fname, self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d876048-f3af-4cbd-b099-a3aa3395020f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact np_dataset:v0, 3816.62MB. 30 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   30 of 30 files downloaded.  \n",
      "Done. 0:0:0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99585b97afa340f8a73601dce9f62f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "artifact = wandb.use_artifact('capecape/gtc/np_dataset:v0', type='dataset')\n",
    "data_artifact_dir = artifact.download()\n",
    "\n",
    "dataset = CloudDataset(\n",
    "    ls(Path(data_artifact_dir)),\n",
    "    num_frames=config.data.num_frames + config.data.num_frames_cond,\n",
    "    size=config.data.image_size\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config.training.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.data.num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "test_x, test_y = next(iter(data_loader))\n",
    "\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cf9558f-2756-434c-ab4d-93d5246f2c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 5, 64, 64]) torch.Size([64, 5, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test_x = data_transform(config, test_x)\n",
    "real, cond, cond_mask = conditioning_fn(\n",
    "    config, test_x,\n",
    "    num_frames_pred=config.data.num_frames,\n",
    "    prob_mask_cond=getattr(config.data, 'prob_mask_cond', 0.0),\n",
    "    prob_mask_future=getattr(config.data, 'prob_mask_future', 0.0)\n",
    ")\n",
    "\n",
    "print(real.shape, cond.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db15d60f-2220-4af0-892e-abc2c86894c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = init_samples(len(real), config)\n",
    "sampler = get_sampler(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0911dfc0-a845-4583-8eee-9a5b74cb0ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPM: 1/100, grad_norm: 143.343017578125, image_norm: 129.55447387695312, grad_mean_norm: 327.7379455566406\n",
      "DDPM: 10/100, grad_norm: 143.14578247070312, image_norm: 130.61659240722656, grad_mean_norm: 321.5141296386719\n",
      "DDPM: 20/100, grad_norm: 143.03793334960938, image_norm: 132.10137939453125, grad_mean_norm: 317.95855712890625\n",
      "DDPM: 30/100, grad_norm: 142.97877502441406, image_norm: 133.49734497070312, grad_mean_norm: 320.0403747558594\n",
      "DDPM: 40/100, grad_norm: 143.80072021484375, image_norm: 134.91636657714844, grad_mean_norm: 318.9917297363281\n",
      "DDPM: 50/100, grad_norm: 146.82933044433594, image_norm: 135.5860137939453, grad_mean_norm: 324.7160949707031\n",
      "DDPM: 60/100, grad_norm: 155.2665557861328, image_norm: 133.97149658203125, grad_mean_norm: 330.50665283203125\n",
      "DDPM: 70/100, grad_norm: 176.037109375, image_norm: 128.89035034179688, grad_mean_norm: 347.5442199707031\n",
      "DDPM: 80/100, grad_norm: 227.30291748046875, image_norm: 120.12443542480469, grad_mean_norm: 364.6986389160156\n",
      "DDPM: 90/100, grad_norm: 383.8355712890625, image_norm: 109.74442291259766, grad_mean_norm: 429.6150207519531\n",
      "DDPM: 100/100, grad_norm: 2549.64990234375, image_norm: 103.42642211914062, grad_mean_norm: 712.6963500976562\n",
      "CPU times: user 9.08 s, sys: 1.48 s, total: 10.6 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred = sampler(\n",
    "    init, scorenet, cond=cond, cond_mask=cond_mask, subsample=100, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57634030-2ecc-4fa5-8dba-7cff0bf343f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/jupyter/mcvd-pytorch/models/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73ae17fddc64f1e8fee50655b17e99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = wandb.Table(\n",
    "    columns=[\n",
    "        \"Initial-Frames\",\n",
    "        \"Predicted-Frames\",\n",
    "        \"Real-Frames\",\n",
    "        \"LPIPS\",\n",
    "        \"Structural-Similarity\",\n",
    "        \"Peak-Signal-To-Noise-Ratio\"\n",
    "    ]\n",
    ")\n",
    "model_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "model_lpips = eval_models.PerceptualLoss(\n",
    "    model='net-lin', net='alex', device=config.device\n",
    ")\n",
    "\n",
    "for idx in tqdm(range(len(real))):\n",
    "    init_images = [\n",
    "        wandb.Image(frame)\n",
    "        for frame in np.expand_dims(init.numpy()[idx], -1)\n",
    "    ]\n",
    "    predicted_images = [\n",
    "        wandb.Image((scale(frame) * 255).astype(np.uint8))\n",
    "        for frame in np.expand_dims(pred.numpy()[idx], -1)\n",
    "    ]\n",
    "    real_images = [\n",
    "        wandb.Image(frame)\n",
    "        for frame in np.expand_dims(real.numpy()[idx], -1)\n",
    "    ]\n",
    "    \n",
    "    psnr_value = 20 * torch.log10(\n",
    "        1.0 / torch.sqrt(F.mse_loss(\n",
    "            scale(torch.from_numpy(real.numpy()[idx])),\n",
    "            scale(torch.from_numpy(pred.numpy()[idx]))\n",
    "        ))\n",
    "    )\n",
    "    \n",
    "    lpis_value, ssim_value = 0, 0\n",
    "    for j in range(config.data.num_frames):\n",
    "        pred_frame = transforms.ToPILImage()(\n",
    "            torch.from_numpy(np.expand_dims(pred.numpy()[idx][j], 0))\n",
    "        ).convert(\"RGB\")\n",
    "        real_frame = transforms.ToPILImage()(\n",
    "            torch.from_numpy(np.expand_dims(real.numpy()[idx][j], 0))\n",
    "        ).convert(\"RGB\")\n",
    "        pred_lpips = model_transforms(pred_frame).unsqueeze(0).to(config.device)\n",
    "        real_lpips = model_transforms(real_frame).unsqueeze(0).to(config.device)\n",
    "        lpis_value += model_lpips.forward(real_lpips, pred_lpips)\n",
    "        \n",
    "        pred_frame_gray = np.asarray(pred_frame.convert('L'))\n",
    "        real_frame_gray = np.asarray(real_frame.convert('L'))\n",
    "        if config.data.dataset.upper() in [\"STOCHASTICMOVINGMNIST\", \"MOVINGMNIST\"]:\n",
    "            pred_frame_gray = np.asarray(\n",
    "                transforms.ToPILImage()(\n",
    "                    torch.round(\n",
    "                        torch.from_numpy(np.expand_dims(pred.numpy()[idx][j], 0))\n",
    "                    )).convert(\"RGB\").convert('L')\n",
    "            )\n",
    "            real_frame_gray = np.asarray(\n",
    "                transforms.ToPILImage()(torch.round(\n",
    "                    torch.from_numpy(np.expand_dims(real.numpy()[idx][j], 0))\n",
    "                )).convert(\"RGB\").convert('L')\n",
    "            )\n",
    "        \n",
    "        ssim_value += structural_similarity(\n",
    "            pred_frame_gray,\n",
    "            real_frame_gray,\n",
    "            data_range=255,\n",
    "            gaussian_weights=True,\n",
    "            use_sample_covariance=False\n",
    "        )\n",
    "    \n",
    "    table.add_data(\n",
    "        init_images, predicted_images, real_images,\n",
    "        lpis_value.item() / float(config.data.num_frames),\n",
    "        ssim_value.item() / float(config.data.num_frames),\n",
    "        psnr_value.item()\n",
    "    )\n",
    "\n",
    "\n",
    "wandb.log({\"Predictions\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fab3372d-da67-4d18-9d89-ced202970f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">virtuous-fuse-93</strong> at: <a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/bhtww66v\" target=\"_blank\">https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/bhtww66v</a><br/>Synced 6 W&B file(s), 1 media file(s), 709 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230130_125830-bhtww66v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c676dcd7-2359-4df2-a55f-f7f5792e09d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
