{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c28d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "\n",
    "import glob\n",
    "import yaml\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from imageio import mimwrite\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "try:\n",
    "    from torchvision.transforms.functional import resize, InterpolationMode\n",
    "    interp = InterpolationMode.NEAREST\n",
    "except:\n",
    "    from torchvision.transforms.functional import resize\n",
    "    interp = 0\n",
    "\n",
    "from main import dict2namespace\n",
    "from models.ema import EMAHelper\n",
    "import models.eval_models as eval_models\n",
    "from models import (\n",
    "    get_sigmas,\n",
    "    anneal_Langevin_dynamics,\n",
    "    anneal_Langevin_dynamics_consistent,\n",
    "    ddpm_sampler,\n",
    "    ddim_sampler,\n",
    "    FPNDM_sampler\n",
    ")\n",
    "from runners.ncsn_runner import get_model, conditioning_fn\n",
    "from load_model_from_ckpt import load_model, get_sampler, init_samples\n",
    "from datasets import get_dataset, data_transform, inverse_data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f267cbe-64d6-4ca2-bc5b-b9ac22d906b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = expanduser(\"~\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d9ac6a-350d-43b4-abaa-89e564f7fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(arr):\n",
    "    m, M = arr.min(), arr.max()\n",
    "    return (arr - m) / (M - m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d956de9-6fba-4824-bfdb-3d36b4178836",
   "metadata": {},
   "source": [
    "# Set directories to download model, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08f0476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THESE!!!\n",
    "GDRIVE_URL = \"https://drive.google.com/drive/folders/1bM6wqU_kymoljz5uYQRCYNup_8adBfLH\" # smmnist_big_5c5_unetm_b2\n",
    "EXP_PATH = os.path.join(home, \"scratch/MCVD_SMMNIST_pred\")\n",
    "DATA_PATH = os.path.join(home, \"scratch/Datasets/MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefab24a",
   "metadata": {},
   "source": [
    "# Download experiment (model checkpoint, config, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef31cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # GDRIVE_URL = GDRIVE_URL.removesuffix(\"?usp=sharing\")\n",
    "# !gdown --fuzzy {GDRIVE_URL} -O {EXP_PATH}/ --folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7497708",
   "metadata": {},
   "source": [
    "# Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eb7d57b-dd89-4b1d-b65e-f131e29361c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgeekyrakshit\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/mcvd-pytorch/wandb/run-20230118_200834-xilv7l9g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/xilv7l9g\" target=\"_blank\">flowing-waterfall-52</a></strong> to <a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion\" target=\"_blank\">https://wandb.ai/wandb/masked-conditional-video-diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/xilv7l9g\" target=\"_blank\">https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/xilv7l9g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact checkpoint-revived-sun-29-1f792ve5:v328, 426.94MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.1\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"masked-conditional-video-diffusion\", entity=\"wandb\", job_type=\"inference\")\n",
    "\n",
    "artifact = wandb.use_artifact(\n",
    "    'wandb/masked-conditional-video-diffusion/checkpoint-revived-sun-29-1f792ve5:v328', type='model'\n",
    ")\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7372e70-7774-4904-b9eb-66bb640a3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, config_path, device=device):\n",
    "    # Parse config file\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    # Load config file\n",
    "    config = dict2namespace(config)\n",
    "    config.device = device\n",
    "    # Load model\n",
    "    scorenet = get_model(config)\n",
    "    if config.device != torch.device('cpu'):\n",
    "        scorenet = torch.nn.DataParallel(scorenet)\n",
    "        states = torch.load(ckpt_path, map_location=config.device)\n",
    "    else:\n",
    "        states = torch.load(ckpt_path, map_location='cpu')\n",
    "        states[0] = OrderedDict([(k.replace('module.', ''), v) for k, v in states[0].items()])\n",
    "    scorenet.load_state_dict(states[0], strict=False)\n",
    "    if config.model.ema:\n",
    "        ema_helper = EMAHelper(mu=config.model.ema_rate)\n",
    "        ema_helper.register(scorenet)\n",
    "        ema_helper.load_state_dict(states[-1])\n",
    "        ema_helper.ema(scorenet)\n",
    "    scorenet.eval()\n",
    "    return scorenet, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bed12d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path = glob.glob(os.path.join(EXP_PATH, \"checkpoint_*.pt\"))[0]\n",
    "ckpt_path = os.path.join(artifact_dir, \"checkpoint.pt\")\n",
    "config_path = \"./smmnist_cat/logs/config.yml\"\n",
    "scorenet, config = load_model(ckpt_path, config_path, device)\n",
    "sampler = get_sampler(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717769fc",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed043f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 60000\n",
      "Dataset length: 256\n"
     ]
    }
   ],
   "source": [
    "dataset, test_dataset = get_dataset(\n",
    "    DATA_PATH, config, video_frames_pred=config.data.num_frames\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "277c205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.training.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.data.num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "test_iter = iter(test_loader)\n",
    "test_x, test_y = next(test_iter)\n",
    "\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cebee4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 5, 64, 64]) torch.Size([64, 5, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test_x = data_transform(config, test_x)\n",
    "real, cond, cond_mask = conditioning_fn(\n",
    "    config, test_x,\n",
    "    num_frames_pred=config.data.num_frames,\n",
    "    prob_mask_cond=getattr(config.data, 'prob_mask_cond', 0.0),\n",
    "    prob_mask_future=getattr(config.data, 'prob_mask_future', 0.0)\n",
    ")\n",
    "\n",
    "print(real.shape, cond.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37ea28",
   "metadata": {},
   "source": [
    "# Load initial samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55bde034",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = init_samples(len(real), config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b62c71",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d2a372b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPM: 1/100, grad_norm: 143.1238555908203, image_norm: 129.24099731445312, grad_mean_norm: 315.03326416015625\n",
      "DDPM: 10/100, grad_norm: 143.14053344726562, image_norm: 130.4148406982422, grad_mean_norm: 323.0744934082031\n",
      "DDPM: 20/100, grad_norm: 143.20840454101562, image_norm: 131.68173217773438, grad_mean_norm: 319.9212951660156\n",
      "DDPM: 30/100, grad_norm: 143.70172119140625, image_norm: 133.24813842773438, grad_mean_norm: 323.2720031738281\n",
      "DDPM: 40/100, grad_norm: 144.8486328125, image_norm: 134.3368377685547, grad_mean_norm: 321.5518493652344\n",
      "DDPM: 50/100, grad_norm: 148.37176513671875, image_norm: 135.39781188964844, grad_mean_norm: 316.765625\n",
      "DDPM: 60/100, grad_norm: 157.77597045898438, image_norm: 136.05177307128906, grad_mean_norm: 322.7695617675781\n",
      "DDPM: 70/100, grad_norm: 180.20025634765625, image_norm: 136.34881591796875, grad_mean_norm: 319.5494689941406\n",
      "DDPM: 80/100, grad_norm: 235.12551879882812, image_norm: 136.62008666992188, grad_mean_norm: 321.6923522949219\n",
      "DDPM: 90/100, grad_norm: 406.3468933105469, image_norm: 136.97003173828125, grad_mean_norm: 320.3355407714844\n",
      "DDPM: 100/100, grad_norm: 3181.64404296875, image_norm: 137.9805908203125, grad_mean_norm: 297.13916015625\n",
      "CPU times: user 9.27 s, sys: 1.45 s, total: 10.7 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred = sampler(\n",
    "    init, scorenet, cond=cond, cond_mask=cond_mask, subsample=100, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2397c10a-a7a1-4240-9da0-0bd0968c97a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/jupyter/mcvd-pytorch/models/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746044bd86a94eaf989143acf6b2627c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = wandb.Table(\n",
    "    columns=[\n",
    "        \"Initial-Frames\",\n",
    "        \"Predicted-Frames\",\n",
    "        \"Real-Frames\",\n",
    "        \"LPIPS\",\n",
    "        \"Structural-Similarity\",\n",
    "        \"Peak-Signal-To-Noise-Ratio\"\n",
    "    ]\n",
    ")\n",
    "model_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "model_lpips = eval_models.PerceptualLoss(\n",
    "    model='net-lin', net='alex', device=device\n",
    ")\n",
    "\n",
    "for idx in tqdm(range(len(real))):\n",
    "    init_images = [\n",
    "        wandb.Image(frame)\n",
    "        for frame in np.expand_dims(init.numpy()[idx], -1)\n",
    "    ]\n",
    "    predicted_images = [\n",
    "        wandb.Image(frame)\n",
    "        for frame in np.expand_dims(pred.numpy()[idx], -1)\n",
    "    ]\n",
    "    real_images = [\n",
    "        wandb.Image(frame)\n",
    "        for frame in np.expand_dims(real.numpy()[idx], -1)\n",
    "    ]\n",
    "    \n",
    "    psnr_value = 20 * torch.log10(\n",
    "        1.0 / torch.sqrt(F.mse_loss(\n",
    "            scale(torch.from_numpy(real.numpy()[idx])),\n",
    "            scale(torch.from_numpy(pred.numpy()[idx]))\n",
    "        ))\n",
    "    )\n",
    "    \n",
    "    lpis_value, ssim_value = 0, 0\n",
    "    for j in range(config.data.num_frames):\n",
    "        pred_frame = transforms.ToPILImage()(\n",
    "            torch.from_numpy(np.expand_dims(pred.numpy()[idx][j], 0))\n",
    "        ).convert(\"RGB\")\n",
    "        real_frame = transforms.ToPILImage()(\n",
    "            torch.from_numpy(np.expand_dims(real.numpy()[idx][j], 0))\n",
    "        ).convert(\"RGB\")\n",
    "        pred_lpips = model_transforms(pred_frame).unsqueeze(0).to(device)\n",
    "        real_lpips = model_transforms(real_frame).unsqueeze(0).to(device)\n",
    "        lpis_value += model_lpips.forward(real_lpips, pred_lpips)\n",
    "        \n",
    "        pred_frame_gray = np.asarray(pred_frame.convert('L'))\n",
    "        real_frame_gray = np.asarray(real_frame.convert('L'))\n",
    "        if config.data.dataset.upper() in [\"STOCHASTICMOVINGMNIST\", \"MOVINGMNIST\"]:\n",
    "            pred_frame_gray = np.asarray(\n",
    "                transforms.ToPILImage()(\n",
    "                    torch.round(\n",
    "                        torch.from_numpy(np.expand_dims(pred.numpy()[idx][j], 0))\n",
    "                    )).convert(\"RGB\").convert('L')\n",
    "            )\n",
    "            real_frame_gray = np.asarray(\n",
    "                transforms.ToPILImage()(torch.round(\n",
    "                    torch.from_numpy(np.expand_dims(real.numpy()[idx][j], 0))\n",
    "                )).convert(\"RGB\").convert('L')\n",
    "            )\n",
    "        \n",
    "        ssim_value += structural_similarity(\n",
    "            pred_frame_gray,\n",
    "            real_frame_gray,\n",
    "            data_range=255,\n",
    "            gaussian_weights=True,\n",
    "            use_sample_covariance=False\n",
    "        )\n",
    "    \n",
    "    table.add_data(\n",
    "        init_images, predicted_images, real_images,\n",
    "        lpis_value.item() / float(config.data.num_frames),\n",
    "        ssim_value.item() / float(config.data.num_frames),\n",
    "        psnr_value.item()\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "initial_videos = [\n",
    "    wandb.Video(video)\n",
    "    for video in np.expand_dims(\n",
    "        (scale(init.numpy()) * 255.0).astype(\"uint8\"), 2\n",
    "    )\n",
    "]\n",
    "    \n",
    "predicted_videos = [\n",
    "    wandb.Video(video)\n",
    "    for video in np.expand_dims(\n",
    "        (scale(pred.numpy()) * 255.0).astype(\"uint8\"), 2\n",
    "    )\n",
    "]\n",
    "\n",
    "real_videos = [\n",
    "    wandb.Video(video)\n",
    "    for video in np.expand_dims(\n",
    "        (scale(real.numpy()) * 255.0).astype(\"uint8\"), 2\n",
    "    )\n",
    "]\n",
    "\n",
    "wandb.log({\n",
    "    # \"Real-Videos\": real_videos,\n",
    "    # \"Initial-Videos\": initial_videos,\n",
    "    # \"Predicted-Videos\": predicted_videos,\n",
    "    \"Predictions\": table\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b39e29d0-dadd-4eeb-bc13-1b410b96016a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flowing-waterfall-52</strong> at: <a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/xilv7l9g\" target=\"_blank\">https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/xilv7l9g</a><br/>Synced 6 W&B file(s), 1 media file(s), 961 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230118_200834-xilv7l9g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
