{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc89438-4ddd-45c6-b863-016a335d1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import (\n",
    "    get_dataset, data_transform, inverse_data_transform\n",
    ")\n",
    "\n",
    "from models import eval_models\n",
    "from models.ema import EMAHelper\n",
    "from models.unet import UNet_SMLD, UNet_DDPM\n",
    "from models.fvd.fvd import (\n",
    "    get_fvd_feats, frechet_distance, load_i3d_pretrained\n",
    ")\n",
    "from models import (\n",
    "    ddpm_sampler,\n",
    "    ddim_sampler,\n",
    "    FPNDM_sampler,\n",
    "    anneal_Langevin_dynamics,\n",
    "    anneal_Langevin_dynamics_consistent,\n",
    "    anneal_Langevin_dynamics_inpainting,\n",
    "    anneal_Langevin_dynamics_interpolation\n",
    ")\n",
    "from models.better.ncsnpp_more import UNetMore_DDPM\n",
    "\n",
    "from losses import get_optimizer, warmup_lr\n",
    "from losses.dsm import anneal_dsm_score_estimation\n",
    "\n",
    "from load_model_from_ckpt import init_samples as initialize_samples\n",
    "from runners.ncsn_runner import conditioning_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db97be8-c9ed-4211-859b-64aa7962bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.channels = 1\n",
    "    config.dataset = 'StochasticMovingMNIST'\n",
    "    config.gaussian_dequantization = False\n",
    "    config.image_size = 64\n",
    "    config.logit_transform = False\n",
    "    config.num_digits = 2\n",
    "    config.num_frames = 5\n",
    "    config.num_frames_cond = 5\n",
    "    config.num_frames_future = 0\n",
    "    config.num_workers = 0\n",
    "    config.prob_mask_cond = 0.0\n",
    "    config.prob_mask_future = 0.0\n",
    "    config.prob_mask_sync = False\n",
    "    config.random_flip = True\n",
    "    config.rescaled = True\n",
    "    config.step_length = 0.1\n",
    "    config.uniform_dequantization = False\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_fast_fid_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.batch_size = 1000\n",
    "    config.begin_ckpt = 5000\n",
    "    config.end_ckpt = 300000\n",
    "    config.ensemble = False\n",
    "    config.freq = 5000\n",
    "    config.n_steps_each = 0\n",
    "    config.num_samples = 1000\n",
    "    config.pr_nn_k = 3\n",
    "    config.step_lr = 0.0\n",
    "    config.verbose = False\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_model_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.arch = 'unetmore'\n",
    "    config.attn_resolutions = [8, 16, 32]\n",
    "    config.ch_mult = [1, 2, 3, 4]\n",
    "    config.cond_emb = False\n",
    "    config.conditional = True\n",
    "    config.depth = 'deep'\n",
    "    config.dropout = 0.1\n",
    "    config.ema = True\n",
    "    config.ema_rate = 0.999\n",
    "    config.gamma = False\n",
    "    config.n_head_channels = 64\n",
    "    config.ngf = 64\n",
    "    config.noise_in_cond = False\n",
    "    config.nonlinearity = 'swish'\n",
    "    config.normalization = 'InstanceNorm++'\n",
    "    config.num_classes = 1000\n",
    "    config.num_res_blocks = 2\n",
    "    config.output_all_frames = False\n",
    "    config.sigma_begin = 0.02\n",
    "    config.sigma_dist = 'linear'\n",
    "    config.sigma_end = 0.0001\n",
    "    config.spade = False\n",
    "    config.spade_dim = 128\n",
    "    config.spec_norm = False\n",
    "    config.time_conditional = True\n",
    "    config.type = 'v1'\n",
    "    config.scheduler = 'DDPM'\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_optim_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.amsgrad = False\n",
    "    config.beta1 = 0.9\n",
    "    config.eps = 1e-08\n",
    "    config.grad_clip = 1.0\n",
    "    config.lr = 0.0002\n",
    "    config.optimizer = 'Adam'\n",
    "    config.warmup = 1000\n",
    "    config.weight_decay = 0.0\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_sampling_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.batch_size = 100\n",
    "    config.ckpt_id = 0\n",
    "    config.clip_before = True\n",
    "    config.consistent = True\n",
    "    config.data_init = False\n",
    "    config.denoise = True\n",
    "    config.fid = False\n",
    "    config.final_only = True\n",
    "    config.fvd = True\n",
    "    config.init_prev_t = -1.0\n",
    "    config.inpainting = False\n",
    "    config.interpolation = False\n",
    "    config.max_data_iter = 100000\n",
    "    config.n_interpolations = 15\n",
    "    config.n_steps_each = 0\n",
    "    config.num_frames_pred = 20\n",
    "    config.num_samples4fid = 10000\n",
    "    config.num_samples4fvd = 10000\n",
    "    config.one_frame_at_a_time = False\n",
    "    config.preds_per_test = 1\n",
    "    config.ssim = True\n",
    "    config.step_lr = 0.0\n",
    "    config.subsample = 1000\n",
    "    config.train = False\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_test_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.batch_size = 100\n",
    "    config.begin_ckpt = 5000\n",
    "    config.end_ckpt = 300000\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_training_configs() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    config.L1 = False\n",
    "    config.batch_size = 64\n",
    "    config.checkpoint_freq = 100\n",
    "    config.log_all_sigmas = False\n",
    "    config.log_freq = 50\n",
    "    config.n_epochs = 10\n",
    "    config.n_iters = 3000001\n",
    "    config.sample_freq = 50000\n",
    "    config.snapshot_freq = 1000\n",
    "    config.snapshot_sampling = True\n",
    "    config.val_freq = 100\n",
    "    config.checkpoint_dir = \"smmnist_cat\"\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_config() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "    \n",
    "    config.data = get_data_configs()\n",
    "    config.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    config.fast_fid = get_fast_fid_configs()\n",
    "    config.model = get_model_configs()\n",
    "    config.optim = get_optim_configs()\n",
    "    config.sampling = get_sampling_configs()\n",
    "    config.test = get_test_configs()\n",
    "    config.training = get_training_configs()\n",
    "    config.start_at = 0\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4853aa36-9029-4274-8a27-034cc91ad7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e3914d-273b-4691-87d7-6f985a35e44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgeekyrakshit\u001b[0m (\u001b[33mwandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/mcvd-pytorch/wandb/run-20230120_140831-qtqnspr7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/qtqnspr7\" target=\"_blank\">genial-spaceship-67</a></strong> to <a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion\" target=\"_blank\">https://wandb.ai/wandb/masked-conditional-video-diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/qtqnspr7\" target=\"_blank\">https://wandb.ai/wandb/masked-conditional-video-diffusion/runs/qtqnspr7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = get_config()\n",
    "config_dict = config.to_dict()\n",
    "config_dict.pop(\"device\", None)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"masked-conditional-video-diffusion\",\n",
    "    entity=\"wandb\",\n",
    "    job_type=\"test\",\n",
    "    config=config_dict\n",
    ")\n",
    "\n",
    "wandb_config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4884700-aad8-4196-aea3-3cd98d6665b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 60000\n",
      "Dataset length: 256\n"
     ]
    }
   ],
   "source": [
    "dataset, test_dataset = get_dataset(\n",
    "    'mnist_dataset/',\n",
    "    config,\n",
    "    video_frames_pred=config.data.num_frames,\n",
    "    start_at=config.start_at\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config.training.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.data.num_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.training.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.data.num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "wandb_config.input_dim = config.input_dim = config.data.image_size ** 2 * config.data.channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26954068-d40b-4529-a634-5725abb5e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorenet = UNetMore_DDPM(config).to(config.device)\n",
    "scorenet = torch.nn.DataParallel(scorenet)\n",
    "optimizer = get_optimizer(config, scorenet.parameters())\n",
    "\n",
    "wandb.log({\n",
    "    \"Parameters\": count_parameters(scorenet),\n",
    "    \"Trainable Parameters\": count_trainable_parameters(scorenet)\n",
    "}, commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62319042-6f9e-4e53-826d-4ea672f0dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs : 1\n",
      "_CudaDeviceProperties(name='NVIDIA A100-SXM4-40GB', major=8, minor=0, total_memory=40354MB, multi_processor_count=108)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs : {num_devices}\")\n",
    "    for i in range(num_devices):\n",
    "        print(torch.cuda.get_device_properties(i))\n",
    "else:\n",
    "    print(f\"Running on CPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab39c46c-7761-4ce4-a187-8926979c1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model.ema:\n",
    "    ema_helper = EMAHelper(mu=config.model.ema_rate)\n",
    "    ema_helper.register(scorenet)\n",
    "\n",
    "net = scorenet.module if hasattr(scorenet, 'module') else scorenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfb2fe57-7573-4e07-9853-1c89a77b5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional = config.data.num_frames_cond > 0\n",
    "cond, test_cond = None, None\n",
    "future = getattr(config.data, \"num_frames_future\", 0)\n",
    "n_init_samples = min(36, config.training.batch_size)\n",
    "init_samples_shape = (\n",
    "    n_init_samples,\n",
    "    config.data.channels * config.data.num_frames,\n",
    "    config.data.image_size,\n",
    "    config.data.image_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee6da114-51bf-4a3e-b9e1-a4f32577c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model.scheduler == \"SMLD\":\n",
    "    init_samples = data_transform(\n",
    "        config,\n",
    "        torch.rand(init_samples_shape, device=config.device)\n",
    "    )\n",
    "elif config.model.scheduler in [\"DDPM\", \"DDIM\", \"FPNDM\"]:\n",
    "    if getattr(config.model, 'gamma', False):\n",
    "        used_k, used_theta = net.k_cum[0], net.theta_t[0]\n",
    "        z = torch.distributions.gamma(\n",
    "            torch.full(init_samples_shape, used_k),\n",
    "            torch.full(init_samples_shape, 1 / used_theta)\n",
    "        ).sample().to(config.device)\n",
    "        init_samples = z - used_k * used_theta\n",
    "    else:\n",
    "        init_samples = torch.randn(init_samples_shape, device=config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada4b689-32b9-4f59-b2ed-4e5a9822b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model.scheduler == \"SMLD\":\n",
    "    consistent = getattr(config.sampling, 'consistent', False)\n",
    "    sampler = anneal_Langevin_dynamics_consistent if consistent else anneal_Langevin_dynamics\n",
    "elif config.model.scheduler == \"DDPM\":\n",
    "    sampler = partial(ddpm_sampler, config=config)\n",
    "elif config.model.scheduler == \"DDIM\":\n",
    "    sampler = partial(ddim_sampler, config=config)\n",
    "elif config.model.scheduler == \"FPNDM\":\n",
    "    sampler = partial(FPNDM_sampler, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa0c4793-a480-4d89-b419-1b185e8cbff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, step):\n",
    "    optimizer.zero_grad()\n",
    "    lr = warmup_lr(\n",
    "        optimizer, step,\n",
    "        getattr(config.optim, 'warmup', 0),\n",
    "        config.optim.lr\n",
    "    )\n",
    "    scorenet.train()\n",
    "    \n",
    "    x = x.to(config.device)\n",
    "    x = data_transform(config, x)\n",
    "    x, cond, cond_mask = conditioning_fn(\n",
    "        config, x, num_frames_pred=config.data.num_frames,\n",
    "        prob_mask_cond=getattr(config.data, 'prob_mask_cond', 0.0),\n",
    "        prob_mask_future=getattr(config.data, 'prob_mask_future', 0.0),\n",
    "        conditional=conditional\n",
    "    )\n",
    "    \n",
    "    loss = anneal_dsm_score_estimation(\n",
    "        scorenet, x, labels=None, cond=cond, cond_mask=cond_mask,\n",
    "        loss_type=getattr(config.training, 'loss_type', 'a'),\n",
    "        gamma=getattr(config.model, 'gamma', False),\n",
    "        L1=getattr(config.training, 'L1', False), hook=None,\n",
    "        all_frames=getattr(config.model, 'output_all_frames', False)\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "        scorenet.parameters(), getattr(config.optim, 'grad_clip', np.inf)\n",
    "    )\n",
    "    optimizer.step()\n",
    "    \n",
    "    if config.model.ema:\n",
    "        ema_helper.update(scorenet)\n",
    "    \n",
    "    return loss.item(), grad_norm.item(), lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4de0b680-bb67-43f9-9a1e-b3e45e01dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(epoch):\n",
    "    test_scorenet = ema_helper.ema_copy(scorenet) if config.model.ema else scorenet\n",
    "    test_scorenet.eval()\n",
    "    x, y = next(iter(test_loader))\n",
    "    x = x.to(config.device)\n",
    "    x = data_transform(config, x)\n",
    "    x, test_cond, test_cond_mask = conditioning_fn(\n",
    "        config, x, num_frames_pred=config.data.num_frames,\n",
    "        prob_mask_cond=getattr(config.data, 'prob_mask_cond', 0.0),\n",
    "        prob_mask_future=getattr(config.data, 'prob_mask_future', 0.0),\n",
    "        conditional=conditional\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        test_dsm_loss = anneal_dsm_score_estimation(\n",
    "            test_scorenet, x, labels=None,\n",
    "            cond=test_cond, cond_mask=test_cond_mask,\n",
    "            loss_type=getattr(config.training, 'loss_type', 'a'),\n",
    "            gamma=getattr(config.model, 'gamma', False),\n",
    "            L1=getattr(config.training, 'L1', False), hook=None,\n",
    "            all_frames=getattr(config.model, 'output_all_frames', False)\n",
    "        )\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                \"validation/epoch\": epoch,\n",
    "                \"validation/loss\": test_dsm_loss.item(),\n",
    "            }, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19522643-5c41-4cd4-93c4-fef866e70ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(scorenet, epoch):\n",
    "    states = [scorenet.state_dict(), optimizer.state_dict(), epoch, step]\n",
    "    if self.config.model.ema:\n",
    "        states.append(ema_helper.state_dict())\n",
    "    checkpoint_path = os.path.join(config.training.checkpoint_dir, 'checkpoint.pt')\n",
    "    torch.save(states, checkpoint_path)\n",
    "    if wandb.run is not None:\n",
    "        artifact = wandb.Artifact(\n",
    "            f'checkpoint-{wandb.run.name}-{wandb.run.id}', type='model'\n",
    "        )\n",
    "        artifact.add_file(checkpoint_path)\n",
    "        wandb.log_artifact(artifact, aliases=[\"latest\", f\"epoch-{epoch}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4fd9b-7bfb-40a6-8eaf-487117736bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69565a56f6c24bdfaf40046021ea8cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "for epoch in range(1, config.training.n_epochs + 1):\n",
    "    train_pbar = tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Training Epoch {epoch}\"\n",
    "    )\n",
    "    for batch, (x, y) in train_pbar:\n",
    "        loss, grad_norm, lr = train_step(x, y, step)\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\n",
    "                \"train/step\": step,\n",
    "                \"lr\": lr,\n",
    "                \"grad_norm\": grad_norm,\n",
    "                \"train/loss\": loss,\n",
    "            }, step=step)\n",
    "            step += 1\n",
    "    save_model(scorenet, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591dbf1-3237-4337-9a1a-0fe7bc6e8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a9fdb-b254-4102-8036-b63c801a4d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
